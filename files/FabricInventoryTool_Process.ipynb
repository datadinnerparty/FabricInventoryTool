{"cells":[{"cell_type":"code","source":["#import necessary libraries\n","import sempy.fabric as fabric\n","import pandas as pd\n","import numpy as np\n","import re\n","from notebookutils import mssparkutils\n","from datetime import date,datetime\n","import time\n","#from pyspark.sql.functions import col, to_date\n","\n","#import sempy_labs as labs\n","#from sempy_labs import directlake\n","#from sempy_labs.tom import connect_semantic_model\n","\n","###########################################################################################################\n","### Set Parameters\n","###########################################################################################################\n","#Define lakehouse and schema names\n","workspace_name = \"amaser_dp700\" #this is the workspace name where the notebook runs, and where the lakehouse lives\n","lakehouse_name = \"test_fit\" #this is the name of the lakehouse, in the event that it needs to becreated by this notebook\n","schema_name = \"fitschema\"  #This is the name of the schema.  if using non-schema-enabled lakehouse, this MUST be dbo\n","semantic_model_name = \"FabricInventory\" ###NOTE: IF THIS MODEL ALREADY EXISTS, IT WILL BE OVERWRITTEN"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:14.5182262Z","session_start_time":null,"execution_start_time":"2026-01-06T01:30:14.5193498Z","execution_finish_time":"2026-01-06T01:30:14.8374313Z","parent_msg_id":"3de15ac5-ff9d-41e3-9d40-4c2913441fb9"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 32, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f459888-e60b-4cd2-b584-0350e02ed875"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96ea3ddb-7026-445a-a84b-f2733ca8487d"},{"cell_type":"code","source":["# Define variables and functions\n","full_lakehouse_name = f'{workspace_name}.{lakehouse_name}.{schema_name}'\n","current_date = date.today()\n","print(current_date)\n","###################################################################################################\n","### DEFINE FUNCTIONS\n","###################################################################################################\n","def logActivity(table_name,message):\n","    logquery = f'INSERT INTO {full_lakehouse_name}.inventorylog (timeval,workspace_id,Message) VALUES (CURRENT_TIMESTAMP(),\"{table_name}\",\"{message}\")'\n","    #print(f'log query={logquery}')\n","    #print(logquery)\n","    spark.sql(logquery)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:14.6748524Z","session_start_time":null,"execution_start_time":"2026-01-06T01:30:14.8395075Z","execution_finish_time":"2026-01-06T01:30:15.144856Z","parent_msg_id":"2a4b9159-5f28-4eaa-ba81-f6e7b7cd36a4"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 33, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2026-01-06\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b62b463-41b2-48e7-9685-a8f9edd9d8f3"},{"cell_type":"code","source":["\n","#Create date table\n","current_year = datetime.now().year\n","end_year = current_year + 1\n","#print(end_year)\n","def create_date_table(start_date='2018-01-01', end_date=f'{end_year}-12-31'):\n","    \"\"\"\n","    Creates a Pandas DataFrame representing a full date table.\n","\n","    Args:\n","        start_date (str): The start date for the date table (format 'YYYY-MM-DD').\n","        end_date (str): The end date for the date table (format 'YYYY-MM-DD').\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame containing various date-related columns.\n","    \"\"\"\n","    df = pd.DataFrame({\"Date\": pd.date_range(start_date, end_date)})\n","\n","    df[\"Day\"] = df.Date.dt.day_name()\n","    df[\"DayOfWeek\"] = df.Date.dt.dayofweek + 1 # Monday=1, Sunday=7\n","    df[\"DayOfMonth\"] = df.Date.dt.day\n","    df[\"DayOfYear\"] = df.Date.dt.dayofyear\n","\n","    df[\"Week\"] = df.Date.dt.isocalendar().week.astype(int)\n","    df[\"WeekOfYear\"] = df.Date.dt.isocalendar().week.astype(int) # Same as Week\n","\n","    df[\"Month\"] = df.Date.dt.month\n","    df[\"MonthName\"] = df.Date.dt.month_name()\n","    df[\"Quarter\"] = df.Date.dt.quarter\n","    df[\"Year\"] = df.Date.dt.year\n","    df[\"YearQuarter\"] = df.Date.dt.strftime('%Y-Q%q')\n","    df[\"Year-Month\"] = df.Date.dt.strftime('%Y-%m')\n","\n","    df[\"IsWeekend\"] = df.Date.dt.dayofweek.isin([5, 6]) # Saturday=5, Sunday=6\n","    df[\"IsLeapYear\"] = df.Date.dt.is_leap_year\n","\n","    return df\n","\n","spark.sql(f\"DROP TABLE IF EXISTS {workspace_name}.{lakehouse_name}.{schema_name}.DimDate\")\n","# Create the date table\n","date_table_df = create_date_table()\n","print(\"DimDate table dropped\")\n","# Display the first few rows of the DataFrame\n","#display(date_table_df.head())\n","#df[\"Year-Month\"] = df[\"Year\"] + \"-\" + df[\"Month\"]\n","#print(df)\n","\n","spark_df = spark.createDataFrame(date_table_df)\n","spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{full_lakehouse_name}.DimDate\")\n","print(\"DiMDate table created\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:14.8092581Z","session_start_time":null,"execution_start_time":"2026-01-06T01:30:15.1470524Z","execution_finish_time":"2026-01-06T01:30:19.2902368Z","parent_msg_id":"cc5d28cf-08b0-4537-b956-80ec7e5dc2e3"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 34, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DimDate table dropped\n"]},{"output_type":"stream","name":"stdout","text":["DiMDate table created\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"e430dad4-fc4d-40dc-9bfc-ebe9c0265301"},{"cell_type":"code","source":["print(\"Starting Workspace table load....\")\n","logActivity(\"\",\"Starting workspace table load\")\n","all_workspaces_df = fabric.list_workspaces()\n","workspace_table_name = f'{full_lakehouse_name}.workspaces'\n","#print (all_workspaces_df.head())\n","workspaces_df = all_workspaces_df[['Name','Id','Is Read Only','Is On Dedicated Capacity','Capacity Id','Type']]\n","#print(workspaces_df)\n","\n","delete_workspace_qeuery = f\"DELETE FROM {workspace_table_name} WHERE timeval = '{current_date}'\"\n","#delete_workspace_qeuery = f\"DELETE FROM {workspace_table_name} \"\n","print(\"Deleting workspace entries for the current date....\")\n","logActivity(\"\",\"Deleting workspace entries for the current date...\")\n","spark.sql(delete_workspace_qeuery)\n","print(\"current date Workspace entries deleted.\")\n","logActivity(\"\",\"current date workspace entries deleted.\")\n","#insert rows into workspace table\n","for index,row in workspaces_df.iterrows():\n","    row_sql = f\"\"\"\n","    INSERT INTO {workspace_table_name} (timeval,workspace_name, workspace_id,is_read_only,is_dedicated_capacity,capacity_id,type) \n","    VALUES ('{current_date}', '{row['Name']}','{row['Id']}',{row['Is Read Only']},{row['Is On Dedicated Capacity']},'{row['Capacity Id']}','Workspace')\n","    \"\"\"\n","    spark.sql(row_sql)\n","print(\"Workspaces inserted\")\n","logActivity(\"\",\"Workspaces inserted\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:14.9305318Z","session_start_time":null,"execution_start_time":"2026-01-06T01:30:19.2923861Z","execution_finish_time":"2026-01-06T01:30:55.5278253Z","parent_msg_id":"945fd40b-dc34-48de-ba79-046cb66ff568"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 35, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting Workspace table load....\n"]},{"output_type":"stream","name":"stdout","text":["Deleting workspace entries....\n"]},{"output_type":"stream","name":"stdout","text":["Workspace entries deleted.\n"]},{"output_type":"stream","name":"stdout","text":["Workspaces inserted\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d77a86db-b4df-4118-b71c-e81fb0aea3ab"},{"cell_type":"code","source":["print(\"Starting fabricItems....\")\n","logActivity(\"\",\"Starting fabricItems\")\n","items_table_name = f'{full_lakehouse_name}.fabricitems'\n","print(\"Deleting current date fabric item entries....\")\n","logActivity(\"\",\"Deleting current date fabric item entriess\")\n","del_items_query = f\"DELETE FROM {items_table_name} WHERE timeval = '{current_date}'\"\n","spark.sql(del_items_query)\n","#display(workspaces_df)\n","rows = workspaces_df[[\"Name\",\"Id\"]]\n","#display(rows)\n","for index,row in rows.iterrows():\n","#for ws in rows:\n","    ws = row['Name']\n","    #print(ws)\n","    items_df = fabric.list_items(workspace=ws)\n","    \n","    items_df.rename(columns={'Display Name':'item_name','Workspace Id':'workspace_id','Id':'item_id','Type':'item_type'},inplace=True)\n","    \n","    del items_df['Folder Id']\n","   \n","    items_df['timeval'] = current_date\n","    #print(items_df)\n","    if items_df.empty:\n","        print(f\"Cannot read data from {ws}\")\n","\n","        logActivity(f'{row[\"Id\"]}',f\"Cannot read data from {ws}\")\n","    else:\n","        items_df = pd.merge(items_df,workspaces_df[['Name','Id']],left_on='workspace_id', right_on='Id',how='left')\n","        items_df = items_df.rename(columns={'Name':'workspace_name'})\n","        del items_df['Id']\n","        spark_df = spark.createDataFrame(items_df)\n","        spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(items_table_name)\n","        print(f'{ws} fabric items entered')\n","        logActivity(f'{row[\"Id\"]}',f'{ws} fabric items entered')\n","    \n","print(\"Current Date fabricItems inserted\")\n","logActivity(\"\",\"Current Date fabricItems inserted.\")\n","     "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:15.0866064Z","session_start_time":null,"execution_start_time":"2026-01-06T01:30:55.5303997Z","execution_finish_time":"2026-01-06T01:31:54.7740855Z","parent_msg_id":"777d7440-89b5-46bc-a185-0856e26c5c38"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 36, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting fabricItems....\n"]},{"output_type":"stream","name":"stdout","text":["Deleting current date fabric item entries....\n"]},{"output_type":"stream","name":"stdout","text":["DIF 1 LAB DEV fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V2 DEV fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V2 TEST fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Cannot read data from Workshop V2 PROD\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V2 WIP fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V3 DEV fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V3 Test fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Cannot read data from Workshop V3 Prod\n"]},{"output_type":"stream","name":"stdout","text":["Workshop V3 MDFL fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700 fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Industry Demos fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["PTurley AzureDevOps Integration fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["MDFL_Next_PTurley fabric items entered\n"]},{"output_type":"stream","name":"stdout","text":["Current Date fabricItems inserted\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"412075ca-2e0e-47e5-9fdf-62c2b553e53f"},{"cell_type":"code","source":["time.sleep(120)\n","fabric.refresh_dataset(workspace=workspace_name, dataset=semantic_model_name)\n","print(\"Dataset refresh completed\")\n","logActivity(\"\",\"Dataset refresh completed\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac","normalized_state":"finished","queued_time":"2026-01-06T01:30:15.1732298Z","session_start_time":null,"execution_start_time":"2026-01-06T01:31:54.7762281Z","execution_finish_time":"2026-01-06T01:34:05.3555447Z","parent_msg_id":"3b5a8685-9415-4ef9-b145-ea806459346a"},"text/plain":"StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 37, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset refresh completed\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b41dee62-aa87-462f-a06d-2b6e7fb536d2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}