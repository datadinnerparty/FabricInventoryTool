{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459888-e60b-4cd2-b584-0350e02ed875",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:30:14.8374313Z",
       "execution_start_time": "2026-01-06T01:30:14.5193498Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3de15ac5-ff9d-41e3-9d40-4c2913441fb9",
       "queued_time": "2026-01-06T01:30:14.5182262Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 32,
       "statement_ids": [
        32
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 32, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from notebookutils import mssparkutils\n",
    "from datetime import date,datetime\n",
    "import time\n",
    "#from pyspark.sql.functions import col, to_date\n",
    "\n",
    "#import sempy_labs as labs\n",
    "#from sempy_labs import directlake\n",
    "#from sempy_labs.tom import connect_semantic_model\n",
    "\n",
    "###########################################################################################################\n",
    "### Set Parameters\n",
    "###########################################################################################################\n",
    "#Define lakehouse and schema names\n",
    "workspace_name = \"<your workspace>\" #this is the workspace name where the notebook runs, and where the lakehouse lives\n",
    "lakehouse_name = \"<your lakehouse>\" #this is the name of the lakehouse, in the event that it needs to becreated by this notebook\n",
    "schema_name = \"<your schema>\"  #This is the name of the schema.  if using non-schema-enabled lakehouse, this MUST be dbo\n",
    "semantic_model_name = \"FabricInventory\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea3ddb-7026-445a-a84b-f2733ca8487d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b62b463-41b2-48e7-9685-a8f9edd9d8f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:30:15.144856Z",
       "execution_start_time": "2026-01-06T01:30:14.8395075Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2a4b9159-5f28-4eaa-ba81-f6e7b7cd36a4",
       "queued_time": "2026-01-06T01:30:14.6748524Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 33,
       "statement_ids": [
        33
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 33, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-06\n"
     ]
    }
   ],
   "source": [
    "# Define variables and functions\n",
    "full_lakehouse_name = f'{workspace_name}.{lakehouse_name}.{schema_name}'\n",
    "current_date = date.today()\n",
    "print(current_date)\n",
    "###################################################################################################\n",
    "### DEFINE FUNCTIONS\n",
    "###################################################################################################\n",
    "def logActivity(table_name,message):\n",
    "    logquery = f'INSERT INTO {full_lakehouse_name}.inventorylog (timeval,workspace_id,Message) VALUES (CURRENT_TIMESTAMP(),\"{table_name}\",\"{message}\")'\n",
    "    #print(f'log query={logquery}')\n",
    "    #print(logquery)\n",
    "    spark.sql(logquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430dad4-fc4d-40dc-9bfc-ebe9c0265301",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:30:19.2902368Z",
       "execution_start_time": "2026-01-06T01:30:15.1470524Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cc5d28cf-08b0-4537-b956-80ec7e5dc2e3",
       "queued_time": "2026-01-06T01:30:14.8092581Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 34,
       "statement_ids": [
        34
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 34, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimDate table dropped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiMDate table created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create date table\n",
    "current_year = datetime.now().year\n",
    "end_year = current_year + 1\n",
    "#print(end_year)\n",
    "def create_date_table(start_date='2018-01-01', end_date=f'{end_year}-12-31'):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame representing a full date table.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): The start date for the date table (format 'YYYY-MM-DD').\n",
    "        end_date (str): The end date for the date table (format 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing various date-related columns.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"Date\": pd.date_range(start_date, end_date)})\n",
    "\n",
    "    df[\"Day\"] = df.Date.dt.day_name()\n",
    "    df[\"DayOfWeek\"] = df.Date.dt.dayofweek + 1 # Monday=1, Sunday=7\n",
    "    df[\"DayOfMonth\"] = df.Date.dt.day\n",
    "    df[\"DayOfYear\"] = df.Date.dt.dayofyear\n",
    "\n",
    "    df[\"Week\"] = df.Date.dt.isocalendar().week.astype(int)\n",
    "    df[\"WeekOfYear\"] = df.Date.dt.isocalendar().week.astype(int) # Same as Week\n",
    "\n",
    "    df[\"Month\"] = df.Date.dt.month\n",
    "    df[\"MonthName\"] = df.Date.dt.month_name()\n",
    "    df[\"Quarter\"] = df.Date.dt.quarter\n",
    "    df[\"Year\"] = df.Date.dt.year\n",
    "    df[\"YearQuarter\"] = df.Date.dt.strftime('%Y-Q%q')\n",
    "    df[\"Year-Month\"] = df.Date.dt.strftime('%Y-%m')\n",
    "\n",
    "    df[\"IsWeekend\"] = df.Date.dt.dayofweek.isin([5, 6]) # Saturday=5, Sunday=6\n",
    "    df[\"IsLeapYear\"] = df.Date.dt.is_leap_year\n",
    "\n",
    "    return df\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {workspace_name}.{lakehouse_name}.{schema_name}.DimDate\")\n",
    "# Create the date table\n",
    "date_table_df = create_date_table()\n",
    "print(\"DimDate table dropped\")\n",
    "# Display the first few rows of the DataFrame\n",
    "#display(date_table_df.head())\n",
    "#df[\"Year-Month\"] = df[\"Year\"] + \"-\" + df[\"Month\"]\n",
    "#print(df)\n",
    "\n",
    "spark_df = spark.createDataFrame(date_table_df)\n",
    "spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{full_lakehouse_name}.DimDate\")\n",
    "print(\"DiMDate table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77a86db-b4df-4118-b71c-e81fb0aea3ab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:30:55.5278253Z",
       "execution_start_time": "2026-01-06T01:30:19.2923861Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "945fd40b-dc34-48de-ba79-046cb66ff568",
       "queued_time": "2026-01-06T01:30:14.9305318Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 35,
       "statement_ids": [
        35
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 35, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Workspace table load....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting workspace entries....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace entries deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspaces inserted\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Workspace table load....\")\n",
    "logActivity(\"\",\"Starting workspace table load\")\n",
    "all_workspaces_df = fabric.list_workspaces()\n",
    "workspace_table_name = f'{full_lakehouse_name}.workspaces'\n",
    "#print (all_workspaces_df.head())\n",
    "workspaces_df = all_workspaces_df[['Name','Id','Is Read Only','Is On Dedicated Capacity','Capacity Id','Type']]\n",
    "#print(workspaces_df)\n",
    "\n",
    "delete_workspace_qeuery = f\"DELETE FROM {workspace_table_name} WHERE timeval = '{current_date}'\"\n",
    "#delete_workspace_qeuery = f\"DELETE FROM {workspace_table_name} \"\n",
    "print(\"Deleting workspace entries for the current date....\")\n",
    "logActivity(\"\",\"Deleting workspace entries for the current date...\")\n",
    "spark.sql(delete_workspace_qeuery)\n",
    "print(\"current date Workspace entries deleted.\")\n",
    "logActivity(\"\",\"current date workspace entries deleted.\")\n",
    "#insert rows into workspace table\n",
    "for index,row in workspaces_df.iterrows():\n",
    "    row_sql = f\"\"\"\n",
    "    INSERT INTO {workspace_table_name} (timeval,workspace_name, workspace_id,is_read_only,is_dedicated_capacity,capacity_id,type) \n",
    "    VALUES ('{current_date}', '{row['Name']}','{row['Id']}',{row['Is Read Only']},{row['Is On Dedicated Capacity']},'{row['Capacity Id']}','Workspace')\n",
    "    \"\"\"\n",
    "    spark.sql(row_sql)\n",
    "print(\"Workspaces inserted\")\n",
    "logActivity(\"\",\"Workspaces inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412075ca-2e0e-47e5-9fdf-62c2b553e53f",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:31:54.7740855Z",
       "execution_start_time": "2026-01-06T01:30:55.5303997Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "777d7440-89b5-46bc-a185-0856e26c5c38",
       "queued_time": "2026-01-06T01:30:15.0866064Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 36,
       "statement_ids": [
        36
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 36, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fabricItems....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current date fabric item entries....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF 1 LAB DEV fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V2 DEV fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V2 TEST fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read data from Workshop V2 PROD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V2 WIP fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V3 DEV fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V3 Test fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read data from Workshop V3 Prod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workshop V3 MDFL fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amaser_dp700 fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry Demos fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTurley AzureDevOps Integration fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDFL_Next_PTurley fabric items entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date fabricItems inserted\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting fabricItems....\")\n",
    "logActivity(\"\",\"Starting fabricItems\")\n",
    "items_table_name = f'{full_lakehouse_name}.fabricitems'\n",
    "print(\"Deleting current date fabric item entries....\")\n",
    "logActivity(\"\",\"Deleting current date fabric item entriess\")\n",
    "del_items_query = f\"DELETE FROM {items_table_name} WHERE timeval = '{current_date}'\"\n",
    "spark.sql(del_items_query)\n",
    "#display(workspaces_df)\n",
    "rows = workspaces_df[[\"Name\",\"Id\"]]\n",
    "#display(rows)\n",
    "for index,row in rows.iterrows():\n",
    "#for ws in rows:\n",
    "    ws = row['Name']\n",
    "    #print(ws)\n",
    "    items_df = fabric.list_items(workspace=ws)\n",
    "    \n",
    "    items_df.rename(columns={'Display Name':'item_name','Workspace Id':'workspace_id','Id':'item_id','Type':'item_type'},inplace=True)\n",
    "    \n",
    "    del items_df['Folder Id']\n",
    "   \n",
    "    items_df['timeval'] = current_date\n",
    "    #print(items_df)\n",
    "    if items_df.empty:\n",
    "        print(f\"Cannot read data from {ws}\")\n",
    "\n",
    "        logActivity(f'{row[\"Id\"]}',f\"Cannot read data from {ws}\")\n",
    "    else:\n",
    "        items_df = pd.merge(items_df,workspaces_df[['Name','Id']],left_on='workspace_id', right_on='Id',how='left')\n",
    "        items_df = items_df.rename(columns={'Name':'workspace_name'})\n",
    "        del items_df['Id']\n",
    "        spark_df = spark.createDataFrame(items_df)\n",
    "        spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(items_table_name)\n",
    "        print(f'{ws} fabric items entered')\n",
    "        logActivity(f'{row[\"Id\"]}',f'{ws} fabric items entered')\n",
    "    \n",
    "print(\"Current Date fabricItems inserted\")\n",
    "logActivity(\"\",\"Current Date fabricItems inserted.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dee62-aa87-462f-a06d-2b6e7fb536d2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-06T01:34:05.3555447Z",
       "execution_start_time": "2026-01-06T01:31:54.7762281Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3b5a8685-9415-4ef9-b145-ea806459346a",
       "queued_time": "2026-01-06T01:30:15.1732298Z",
       "session_id": "fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 37,
       "statement_ids": [
        37
       ]
      },
      "text/plain": [
       "StatementMeta(, fe5d2027-5ec5-4f1b-b950-0f5fc334a6ac, 37, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset refresh completed\n"
     ]
    }
   ],
   "source": [
    "time.sleep(120)\n",
    "fabric.refresh_dataset(workspace=workspace_name, dataset=semantic_model_name)\n",
    "print(\"Dataset refresh completed\")\n",
    "logActivity(\"\",\"Dataset refresh completed\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
